<!doctype html><html><head>  <meta charset="utf-8">  <meta name="keywords" content="">  <meta name="description" content="">  <meta name="robots" content="index, follow">  <link rel="shortcut icon" type="image/svg" href="../favicon.svg">  <link rel="stylesheet" type="text/css" href="../css/bootstrap.min.css?524"><link rel="stylesheet" type="text/css" href="../style.css?8344"><link rel="stylesheet" type="text/css" href="../css/animate.min.css?6145"><link rel="stylesheet" type="text/css" href="../css/tables.css?1352"><link rel="stylesheet" type="text/css" href="../css/all.min.css"><link rel="stylesheet" type="text/css" href="../css/ionicons.min.css">  <title>Workshop-Plot-Data-1</title>  <!-- Analytics --> <!-- Analytics END -->  </head><body data-clean-url="true"><!-- Preloader --><div id="page-loading-blocs-notifaction" class="page-preloader"></div><!-- Preloader END --><!-- Main container --><div class="page-container">  <!-- navbar --><div class="bloc l-bloc" id="navbar"><div class="container bloc-no-padding-lg bloc-no-padding-md bloc-no-padding-sm bloc-no-padding"><div class="row"><div class="order-md-0 offset-sm-0 text-start align-self-center text-sm-start offset-0 col-lg-9 navs-controls col-md-9 col-12 order-1 col-sm-9 order-sm-0"><nav class="navbar navbar-light row navbar-expand-md" role="navigation"><div class="container-fluid"><a class="navbar-brand animated pulse-hvr" href="../" target="_blank"><picture><source type="image/webp" srcset="../img/bioinvasion_logo_500x500_BW.webp"><img src="../img/bioinvasion_logo_500x500_BW.png" alt="logo" class="me-2" width="36" height="36"></picture>Researcher</a></div></nav><nav class="navbar navbar-light row navbar-expand-md pt-0 pb-0 nav-invert hover-open-submenu"><div class="container-fluid nav-invert"><button id="nav-toggle" type="button" class="ui-navbar-toggler navbar-toggler border-0 p-0" aria-expanded="false" aria-label="Toggle navigation" data-bs-toggle="collapse" data-bs-target=".navbar-17897"><span class="navbar-toggler-icon"><svg height="32" viewBox="0 0 32 32" width="32"><path class="svg-menu-icon  " d="m2 9h28m-28 7h28m-28 7h28"></path></svg></span></button><div class="collapse navbar-collapse navbar-19859 navbar-17897  sidebar-nav"><ul class="site-navigation nav navbar-nav"><li class="nav-item"><div class="dropdown btn-dropdown dropdown"><a href="#" class="nav-link nav-links dropdown-toggle" data-bs-toggle="dropdown" aria-expanded="false" tabindex="0">Research</a><ul class="dropdown-menu" role="menu"><li class="nav-item"><a href="../publications/" class="nav-link nav-list-links">Publications</a></li><li class="nav-item"><a href="../presentations/" class="nav-link nav-list-links">Presentations</a><div class="devmargin"><div class="divider-h nav-divider-0"></div></div></li><li class="nav-item"><a href="../research-archive/" class="nav-link nav-list-links">All Posts</a></li></ul></div></li><li class="nav-item"><div class="dropdown btn-dropdown dropdown"><a href="#" class="nav-link nav-links dropdown-toggle" data-bs-toggle="dropdown" aria-expanded="false" tabindex="0">Engagement</a><ul class="dropdown-menu" role="menu"><li class="nav-item"><a href="../education/" class="nav-link nav-list-links">Education</a></li><li class="nav-item"><a href="../side-projects/" class="nav-link nav-list-links">Side Projects</a></li><li class="nav-item"><div class="devmargin"><div class="divider-h nav-divider-0"></div></div></li><li class="nav-item"><a href="../engagement-archive/" class="nav-link nav-list-links">All Posts</a></li></ul></div></li><li class="nav-item"><div class="dropdown btn-dropdown dropdown"><a href="#" class="nav-link nav-links dropdown-toggle" data-bs-toggle="dropdown" aria-expanded="false" tabindex="0">Projects</a><ul class="dropdown-menu" role="menu"><li class="nav-item"><a href="../projasaas/" class="nav-link nav-list-links">ASAAS</a></li><li class="nav-item"><a href="../projagriweed/" class="nav-link nav-list-links">AgriWeedClim</a></li></ul></div></li><li class="nav-item"><a href="https://www.gillescolling.com/blog" class="nav-link nav-links link-about-style">Blog</a></li><li class="nav-item"><a href="../contact/" class="nav-link nav-links link-about-style">Contact</a></li><li class="nav-item"><a href="../author/" class="nav-link nav-links link-about-style">About</a></li></ul></div></div></nav></div><div class="col-lg-3 col-md-3 text-start order-sm-0 col-12 order-0 nav-icon-background align-self-center mb-md-1 mb-lg-1 mt-sm-0 mt-0 col-sm-3"><div><div class="social-link-bric" data-aria="true"><a href="https://twitter.com/Gilles__Colling" class="twitter-link" target="_blank" aria-label="Twitter"><svg xmlns="http://www.w3.org/2000/svg" width="46" height="46" fill="#212121" viewBox="0 0 24 24" style="margin-left: 4px; margin-right: 4px;"><path d="m19 24h-14a5 5 0 0 1 -5-5v-14a5 5 0 0 1 5-5h14a5 5 0 0 1 5 5v14a5 5 0 0 1 -5 5zm.053-18.993h-2.453l-4.047 4.468-3.493-4.468h-5.06l6.047 7.646-5.734 6.337h2.454l4.426-4.89 3.867 4.89h4.94l-6.307-8.059zm-3.313 12.565-8.873-11.218h1.46l8.773 11.218z"></path></svg></a><a href="https://linkedin.com/in/gilles-colling-0b3747306/ " class="linkedin-link" target="_blank" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="46" height="46" fill="#212121" viewBox="0 0 24 24" style="margin-left: 4px; margin-right: 4px;"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"></path></svg></a><a href="https://bsky.app/profile/gilles-colling.bsky.social" class="bluesky-link" target="_blank" aria-label="Bluesky"><svg xmlns="http://www.w3.org/2000/svg" fill="#212121" width="46" height="46" viewBox="0 0 24 24" style="margin-left: 4px; margin-right: 4px;"><path d="m19,24H5c-2.76,0-5-2.24-5-5V5C0,2.24,2.24,0,5,0h14c2.76,0,5,2.24,5,5v14c0,2.76-2.24,5-5,5Zm-2.08-18.61c-1.99,1.49-4.12,4.51-4.91,6.14-.79-1.62-2.92-4.64-4.91-6.14-1.43-1.08-3.75-1.91-3.77.74,0,.53.3,4.45.48,5.08.62,2.21,2.87,2.77,4.88,2.43-3.5.6-4.4,2.57-2.47,4.55,3.66,3.75,5.26-.94,5.67-2.14.07-.22.11-.32.11-.24,0-.09.04.01.11.24.41,1.21,2.01,5.9,5.67,2.14,1.93-1.98,1.03-3.95-2.47-4.55,2.01.34,4.26-.22,4.88-2.43.18-.64.48-4.56.48-5.08,0-2.65-2.32-1.82-3.75-.74Z"></path></svg></a><a href="https://www.researchgate.net/profile/Gilles-Colling" class="researchgate-link" target="_blank"><svg fill="#212121" width="46" height="46" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" style="margin-left: 4px; margin-right: 4px;"><path d="M 5 0 C 2.2400028 0 0 2.2400028 0 5 L 0 19 C 0 21.759997 2.2400028 24 5 24 L 19 24 C 21.759997 24 24 21.759997 24 19 L 24 5 C 24 2.2400028 21.759997 0 19 0 L 5 0 z M 17.689453 3 A 3.051 3.051 0 0 1 18.546875 3.1113281 A 2.48475 2.48475 0 0 1 19.222656 3.4179688 A 2.25 2.25 0 0 1 19.732422 3.8730469 A 2.66175 2.66175 0 0 1 20.082031 4.4296875 C 20.118031 4.4926874 20.107031 4.5440782 20.050781 4.5800781 L 19.267578 4.9023438 C 19.202328 4.9383437 19.154391 4.9201093 19.119141 4.8496094 C 18.942141 4.5248597 18.794375 4.3114998 18.546875 4.15625 C 18.300125 4.0010002 18.0685 3.9277344 17.6875 3.9277344 C 17.2795 3.9277344 17.093937 4.0103596 16.835938 4.1933594 A 1.4175 1.4175 0 0 0 16.279297 4.9492188 A 2.3925 2.3925 0 0 0 16.167969 5.5195312 A 11.86575 11.8665 0 0 0 16.140625 6.0117188 A 19.80075 19.80225 0 0 0 16.130859 6.7148438 C 16.130859 6.9900935 16.133125 7.2249221 16.140625 7.4199219 A 13.0245 13.02525 0 0 0 16.167969 7.9121094 A 2.352 2.352 0 0 0 16.277344 8.4824219 C 16.392094 8.8281715 16.549797 9.017672 16.779297 9.1699219 C 17.011047 9.3221717 17.27875 9.4335938 17.6875 9.4335938 C 18.05425 9.4335938 18.389922 9.3308748 18.607422 9.171875 C 18.824922 9.0128752 19.004938 8.8043747 19.085938 8.515625 C 19.121187 8.3963751 19.166813 8.2435467 19.195312 8.0605469 C 19.222312 7.8767971 19.222656 7.6821091 19.222656 7.4121094 C 19.222656 7.3701094 19.202156 7.3476562 19.160156 7.3476562 L 17.910156 7.3476562 C 17.839656 7.3476563 17.806641 7.3126874 17.806641 7.2421875 L 17.806641 6.5253906 C 17.806641 6.4548907 17.839656 6.4199219 17.910156 6.4199219 L 20.208984 6.4199219 C 20.278734 6.4199219 20.314453 6.4541407 20.314453 6.5253906 L 20.314453 7.140625 A 8.175 8.175 0 0 1 20.28125 8.046875 C 20.24975 8.3258747 20.205437 8.5678908 20.148438 8.7441406 C 19.972188 9.2946401 19.695078 9.687969 19.267578 9.9804688 C 18.840829 10.272968 18.289453 10.433594 17.689453 10.433594 C 17.075204 10.433594 16.556562 10.291765 16.132812 10.009766 C 15.709813 9.7270159 15.406406 9.3072494 15.222656 8.75 A 2.5005 2.5005 0 0 1 15.136719 8.421875 L 15.140625 8.421875 A 5.9595 5.9595 0 0 1 15.083984 8.0234375 A 6.75 6.75 0 0 1 15.044922 7.4785156 A 23.34075 23.34225 0 0 1 15.035156 6.7167969 C 15.035156 6.4205472 15.037422 6.1665779 15.044922 5.9550781 A 6.75 6.75 0 0 1 15.082031 5.4082031 A 6.27375 6.27375 0 0 1 15.140625 5.0117188 A 2.39475 2.39475 0 0 1 15.224609 4.6835938 C 15.406859 4.1255943 15.712516 3.7065778 16.134766 3.4238281 C 16.558515 3.1425784 17.075954 3 17.689453 3 z M 9.1621094 7.3769531 L 9.1640625 7.3769531 C 11.61356 7.3769531 13.482422 8.494393 13.482422 10.900391 C 13.482422 12.533889 12.19117 14.167422 10.451172 14.576172 C 11.504921 16.44517 12.773751 18.09886 13.71875 19.130859 C 14.277499 19.732359 15.05097 20.269531 15.824219 20.269531 L 15.824219 20.828125 C 15.587969 20.936125 15.201187 21 14.835938 21 C 13.589439 21 12.602031 20.506421 11.957031 19.732422 C 11.248282 18.916423 10.000452 17.154748 8.6894531 14.8125 C 8.0024538 14.8125 7.5511557 14.812281 7.0351562 14.769531 L 7.0351562 18.357422 C 7.0351563 19.690171 7.3362662 19.968438 7.9160156 20.054688 L 9.0332031 20.205078 L 9.0332031 20.828125 C 8.4962037 20.806375 7.2507022 20.763672 6.2832031 20.763672 C 5.2744541 20.763672 4.2652963 20.806375 3.6855469 20.828125 L 3.6855469 20.205078 L 4.5 20.054688 C 5.0579994 19.947438 5.3808594 19.690171 5.3808594 18.357422 L 5.3808594 9.890625 C 5.3808594 8.5586263 5.0579994 8.3013593 4.5 8.1933594 L 3.6855469 8.0429688 L 3.6855469 7.4199219 C 4.2225463 7.4416719 5.079954 7.4414062 5.9394531 7.4414062 C 6.8852022 7.4414062 7.8946106 7.3769531 9.1621094 7.3769531 z M 8.7753906 8.34375 C 8.1956412 8.34375 7.5301558 8.3444219 7.0351562 8.3886719 L 7.0351562 13.78125 C 7.5294058 13.82325 7.851532 13.845703 8.5820312 13.845703 C 10.494529 13.845703 11.65625 12.705061 11.65625 11.007812 C 11.65625 9.2888142 10.581842 8.34375 8.7773438 8.34375 L 8.7753906 8.34375 z"></path></svg></a></div></div></div></div></div></div><!-- navbar END --><!-- bloc-143 --><div class="bloc l-bloc" id="bloc-143"><div class="container bloc-sm-lg bloc-sm-md bloc-sm-sm bloc-sm"><div class="row"><div class="text-start col-12"><div class="dev-nav-link"><a href="../" class="a-btn text-link-01">Home</a><span class="ion ion-chevron-right arrow-arrow"></span><a href="../engagement-archive/" class="a-btn text-link-01">Engagement</a><span class="ion ion-chevron-right arrow-arrow"></span><a href="../education/" class="a-btn text-link-01">Education</a><span class="ion ion-chevron-right arrow-arrow"></span><a href="." class="a-btn text-link-01">BART Workshop</a></div></div></div></div></div><!-- bloc-143 END --><!-- bloc-144 --><div class="bloc l-bloc" id="bloc-144"><div class="container bloc-no-padding-lg bloc-no-padding-md bloc-no-padding-sm bloc-no-padding"><div class="row"><div class="text-start col-md-10 offset-md-1 offset-lg-0 col-lg-12"><h1 class="display-md text-bold">BART (Bayesian Additive Regression Trees) Workshop</h1><div class="mt-3 writer-post-group mb-3"><p class="mb-0 p-sm">Sep 22-24, 2025</p><a href="../education/" class="a-btn text-link-01 post-label ">Education</a></div><p>From September 22 to 24, 2025, Jeremy Yoder and Colin Carlson led a workshop on Bayesian Additive Regression Trees (BART) and their application to species distribution modeling. The sessions combined conceptual explanations with hands-on coding, using the R packages <a class="text-link-01" href="https://github.com/cjcarlson/embarcadero">`embarcadero`</a> and <a class="text-link-01" href="https://cran.r-project.org/web/packages/dbarts/index.html">`dbarts`</a>. We worked through a complete workflow, starting from classical species distribution models and moving toward Bayesian tree ensembles with hierarchical extensions. The workshop did not require prior experience with BART or boosted trees, but a basic understanding of regression models in R was expected.</p></div></div></div></div><!-- bloc-144 END --><!-- bloc-145 --><div class="bloc bgc-4819 bg-bart-modelling2 d-bloc bloc-bg-texture texture-darken-strong" id="bloc-145"><div class="bloc-shape-divider bloc-divider-bottom"><svg class="svg-divider bloc-divider-b-0-fill" fill-rule="evenodd" preserveAspectRatio="none" viewBox="0 0 1000 250"><path d="M1000,0V250H0v-1Z"></path></svg></div><div class="container bloc-md bloc-lg-md bloc-lg-lg"><div class="row"><div class="col-md-10 offset-md-1 offset-lg-0 col-lg-12"><h2 class="mb-4 display-md tc-4629 text-bold">Take aways</h2><div><p class="tc-4629">Across three days, the workshop built a continuous path from classical regression models to Bayesian tree ensembles, connecting ecological intuition with modern statistical tools. Through a mix of lectures and live coding in R, we worked step by step from basic species distribution models toward BART and its random-intercept extensions. Using the Joshua tree (<i>Yucca brevifolia</i>) dataset as a running example, we compared how different modeling approaches capture environmental structure across the southwestern United States. <br><br>Each session deepened the same idea: ecological responses are nonlinear, interactive, and uncertain, and modeling should reflect this complexity rather than simplify it away. BART provided a natural framework for that goal, combining the flexibility of decision trees with the interpretability of regression and the rigor of Bayesian inference. <br><br>We learned BART modeling as a way to move beyond the constraints of linear models, using ensembles of regression trees to uncover nonlinear relationships, capture interactions automatically, and quantify uncertainty while accounting for regional structure in the data.<br></p></div></div></div></div></div><!-- bloc-145 END --><!-- bloc-146 --><div class="bloc l-bloc" id="bloc-146"><div class="container bloc-lg bloc-no-padding-lg"><div class="row"><div class="col text-start"><h1 class="mb-4 text-lg-center mt-lg-4 mb-lg-3">Species Distribution Modeling (SDM)</h1><p>The first lecture set the foundation for everything that followed. Before jumping into Bayesian frameworks or tree ensembles, we revisited the basic structure of species distribution models (SDMs) and what they actually estimate. The guiding question was simple: <i>given a set of environmental conditions, what is the probability that a species occurs at a given site?</i><br></p><h2 class="mb-4">Model structure</h2><p>At its simplest, an SDM models a binary response variable \(Y_i\), representing presence (1) or absence (0) at site \(i\), as a function of environmental predictors \(\mathbf{X}_i\). In classical form:<br>\[Y_i \sim \text{Bernoulli}(p_i)\]<br>and<br>\[\text{logit}(p_i) = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots + \beta_k X_{ki},\]<br>where \(\beta_0\) is the intercept, \(\beta_j\) are the coefficients for predictors \(X_j\), and \(p_i\) is the estimated probability of presence. <br><br>This logistic regression framework assumes that each predictor contributes linearly on the logit scale and that effects combine additively. Both assumptions are restrictive for ecological data, where predictors often interact or have nonlinear responses.<br></p><h2 class="mb-4">Presence/Absence and Pseudo-Absences</h2><p>The Joshua tree (<i>Yucca brevifolia</i>) dataset provided an ideal test case. True absence data were unavailable, so pseudo-absences were generated by randomly sampling points outside known occurrences. This allowed us to fit a model despite incomplete detection. The response variable \(Y_i\) thus reflected both presences and pseudo-absences, forming a quasi-Bernoulli sample.<br><br>However, pseudo-absences introduce a sampling bias because the background distribution does not necessarily represent <strong>true</strong>&nbsp;absences. If pseudo-absences are drawn from regions with distinct environmental conditions, the estimated coefficients \(\beta_j\) may be skewed. The instructors emphasized that understanding this bias is crucial when interpreting parameter estimates or predicted probabilities.<br></p><h2 class="mb-4">Environmental Predictors</h2><p>Predictors were derived from <strong>BioClim</strong> climatic layers, for our modelling set we used:<br></p><ul><li><p>Mean annual temperature (\(\text{BIO1}\))<br></p></li><li><p>Annual precipitation (\(\text{BIO12}\))</p></li><li><p>Temperature seasonality (\(\text{BIO4}\))</p></li><li><p>Precipitation of the driest quarter (\(\text{BIO17}\))</p></li></ul><p class="mb-3">Each \(X_j\) was scaled before fitting, making coefficients \(\beta_j\) comparable in magnitude. We also discussed the ecological meaning of each variable. For instance, \(\text{BIO12}\) often reflects water limitation, while \(\text{BIO4}\) can indicate sensitivity to temperature variability.</p><h2 class="mb-4">Interpretation</h2><p>Once the model was fitted, predicted probabilities \(\hat{p}_i\) were visualized across environmental gradients:<br>\[\hat{p}_i = \frac{1}{1 + e^{-(\beta_0 + \sum_j \beta_j X_{ji})}}\]<br>Plotting \(\hat{p}_i\) against temperature or precipitation revealed typical sigmoidal relationships, highlighting thresholds where probability of occurrence changes sharply. Even when responses were clearly nonlinear (for example, tolerance to intermediate temperature ranges) the logit-linear assumption forced the model into overly simple patterns.<br></p><h2 class="mb-4">Motivation for Flexible Models</h2><p>To address these limitations, the instructors introduced the idea of <strong>tree-based methods</strong>. Unlike regression, which assumes a fixed functional form, tree models partition predictor space recursively, allowing \(p_i\) to change arbitrarily across regions of \(\mathbf{X}\). Formally, a decision tree can be expressed as a step function:<br>\[f(\mathbf{X}) = \sum_{m=1}^{M} c_m \, I(\mathbf{X} \in R_m),\]<br>where \(R_m\) are partitions of the predictor space and \(c_m\) are fitted constants. Ensemble methods such as <strong>Boosted Regression Trees (BRTs)</strong> or <strong>Random Forests</strong>&nbsp;combine many such trees to produce smoother, more stable predictions. We compared logistic regression and BRT predictions for the same dataset. The BRT captured nonlinear responses and variable interactions automatically, setting the conceptual bridge toward <strong>BART</strong>, where this flexibility is embedded within a Bayesian framework.<br><br></p><h1 class="mb-4 text-lg-center mt-lg-4 mb-lg-3">Bayesian Modeling<br></h1><p>The second lecture introduced the Bayesian framework that underpins BART. The shift from classical to Bayesian modeling wasn’t presented as a new method but as a new way of reasoning about models: treating parameters as random variables and uncertainty as an integral part of inference.<br></p><h2 class="mb-4">The Bayesian Framework</h2><p>In the Bayesian approach, all unknown quantities, coefficients, predictions, even latent processes, are treated as random variables with probability distributions. The goal is to update prior beliefs using data according to Bayes’ theorem:<br>\[p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{p(y)},\]<br>where<br></p><ul><li><p>\(p(\theta)\) is the <strong>prior</strong>, representing beliefs about parameters before seeing the data,</p></li><li><p>\(p(y \mid \theta)\) is the <strong>likelihood</strong>, describing how data are generated given parameters,</p></li><li><p>\(p(\theta \mid y)\) is the <strong>posterior</strong>,&nbsp;the updated belief after observing data,</p></li><li><p>\(p(y)\) is the <strong>marginal likelihood</strong>, a normalizing constant.</p></li></ul><p>Inference is based on the full posterior distribution rather than on point estimates such as \(\hat{\beta}\) or \(\hat{p}\). Every model quantity carries an explicit measure of uncertainty.<br></p><h2 class="mb-4">From Likelihood to Posterior</h2><p>In logistic regression, the likelihood for \(n\) independent observations is:<br>\[p(\mathbf{y} \mid \boldsymbol{\beta}) = \prod_{i=1}^n p_i^{y_i} (1 - p_i)^{1 - y_i},\]<br>with <br>\[\text{logit}(p_i) = \beta_0 + \sum_{j=1}^{k} \beta_j X_{ji}.\]<br>Assuming a normal prior \(\beta_j \sim \mathcal{N}(0, \sigma^2_\beta)\) for each coefficient, the posterior becomes:<br>\[p(\boldsymbol{\beta} \mid \mathbf{y}) \propto p(\mathbf{y} \mid \boldsymbol{\beta}) \, p(\boldsymbol{\beta})\]<br>Since this posterior cannot be derived analytically, it’s typically approximated using Markov Chain Monte Carlo (MCMC) methods, drawing samples \(\boldsymbol{\beta}^{(1)}, \ldots, \boldsymbol{\beta}^{(S)}\) from \(p(\boldsymbol{\beta} \mid \mathbf{y})\). These samples form the basis for estimating means, credible intervals, and predictive distributions.<br></p><h2 class="mb-4">Application to Species Distribution</h2><p>In the SDM context, Bayesian inference allows us to model detection uncertainty, account for imperfect data, and incorporate spatial or hierarchical structure. For example, random effects for regions or observers can be introduced with their own priors:<br>\[\beta_{0r} \sim \mathcal{N}(\mu_0, \sigma^2_r), \]<br>where \(\beta_{0r}\) is the region-specific intercept and \(\sigma^2_r\) controls variability across regions. Such extensions are central to hierarchical Bayesian models and later reappear in <strong>random-intercept BART</strong>.<br></p><h2 class="mb-4">From Parametric to Flexible Priors</h2><p>The final part of the session connected Bayesian regression to the motivation for tree-based models. In a standard logistic regression, the prior structure is simple: typically independent normals on \(\beta_j\). But for nonlinear and interactive effects, defining appropriate priors becomes difficult.<br><br>Tree-based Bayesian models, like BART, sidestep this by <strong>placing priors</strong> on <strong>functions</strong> rather than individual parameters. Instead of \(\beta_j\), BART defines priors over trees that partition predictor space and contribute small additive effects. These functional priors allow the model to learn nonlinearities while preserving a coherent probabilistic interpretation.<br><br>By the end of the lecture, the group had implemented a simple Bayesian logistic regression in R using <a class="text-link-01" href="https://cran.r-project.org/web/packages/brms/index.html">` brms`</a> , compared posterior distributions of coefficients, and visualized predictive uncertainty. It provided the conceptual bridge between regression models and the probabilistic logic that BART extends to high-dimensional nonlinear problems.</p><h1 class="mb-4 text-lg-center mt-lg-4 mb-lg-3">Bayesian Additive Regression Trees (BART)<br></h1><p>The third lecture introduced the central method of the workshop: Bayesian Additive Regression Trees (BART). To understand it, we first revisited how a single regression tree works. Then we built up to the idea of combining many of them under a Bayesian framework.</p><h2 class="mb-4">Regression Trees</h2><p>A <strong>regression tree</strong> models a response \(y_i\) by recursively partitioning the predictor space into regions where the response is approximately constant. Conceptually, it works like a structured game of “20 questions”. At each step, the model asks a yes-or-no question about one predictor variable and divides the data into two groups that are more homogeneous in their response values.<br><br>Formally, consider a dataset \(\{(\mathbf{X}_i,y_i)\}_{i=1}^n\) with predictors \(\mathbf{X}_i=(X_{i1},X_{i2},\ldots,X_{ip})\). The goal is to partition the predictor space into \(M\) disjoint regions \(R_1,R_2,\ldots,R_M\), such that within each region \(R_m\), the response \(y_i\) is well approximated by a constant \(\mu_m\). The model is defined as:<br>\[f(\mathbf{X}_i)=\sum_{m=1}^{M}\mu_m I(\mathbf{X}_i\in R_m),\]<br>where \(I(\cdot)\) is an indicator function equal to 1 if \(\mathbf{X}_i\) belongs to region \(R_m\) and 0 otherwise. The parameters of this model are both the region boundaries (the <i>splits)</i>&nbsp;and the terminal node means \(\mu_m\). To build the tree, we choose the splits that minimize the <strong>residual sum of squares (RSS)</strong>:<br>\[\text{RSS} = \sum_{m=1}^{M} \sum_{i \in R_m} (y_i - \mu_m)^2\]<br>At each step, the algorithm considers every variable \(j\( and every possible split point \(s\(, dividing the data into two regions:<br>\[R_1(j, s) = \{ \mathbf{X} \mid X_j &lt; s \}, \qquad R_2(j, s) = \{ \mathbf{X} \mid X_j \ge s \}.\]<br>For each candidate split, we compute the resulting total error:<br>\[E(j, s) = \sum_{i \in R_1(j, s)} (y_i - \bar{y}_{R_1})^2 + \sum_{i \in R_2(j, s)} (y_i - \bar{y}_{R_2})^2\]<br>and choose \((j^*, s^*)\) that minimizes \(E(j, s)\). This greedy algorithm continues recursively, fitting new splits within each region until a stopping criterion is reached, typically a minimum node size or maximum depth.<br>Each region \(R_m\) thus defines a rectangular subset of predictor space, and each \(\mu_m = \bar{y}_{R_m}\) is simply the average response within that subset. In geometric terms, the fitted function \(f(\mathbf{X})\) is a <strong>piecewise-constant approximation</strong> of the true regression surface, flat within each region, with discontinuities at split boundaries.<br><br>Intuitively, each split can be seen as an ecological decision rule. The first split might divide “cool” from “warm” climates based on a temperature threshold (e.g., \(X_{\text{temp}} &lt; 15\)°C). Within each branch, the next split might depend on precipitation (e.g., \(X_{\text{precip}} &gt; 400\) mm), producing finer subdivisions of environmental space. The process continues until every observation belongs to one of \(M\) homogeneous “niches,” each characterized by a distinct mean response.<br><br>This recursive partitioning has two important implications. <br><br>First, it allows <strong>automatic interaction modeling:</strong>&nbsp;the effect of one variable can depend on previous splits. Temperature may only matter under certain precipitation regimes, or vice versa, without needing to specify interactions manually. <br><br>Second, it provides a <strong>nonparametric&nbsp;</strong>representation: the model makes no assumptions about linearity or additivity, only that locally similar conditions yield similar responses. <br><br>However, single trees are prone to overfitting and instability. Because each split depends on the previous ones, small data perturbations can lead to very different trees. The fitted function is discontinuous, jumping sharply at region boundaries, and may not generalize well outside the training sample. Despite these limitations, regression trees remain the conceptual foundation of ensemble methods like Random Forests, Boosted Regression Trees, and Bayesian Additive Regression Trees (BART). All of them retain the core idea of recursively partitioning predictor space, but they combine many trees to produce smoother, more stable, and often more interpretable models.</p><h2 class="mb-4">From Trees to Ensembles</h2><p>To stabilize predictions, tree-based ensembles like <strong>Random Forests</strong> and <strong>Boosted Regression Trees (BRTs)</strong> combine many trees into a smoother model. Random forests average predictions from many independent trees, while boosting adds trees sequentially to model residuals left by previous ones.<br><br>The general ensemble form is:<br>\[f(\mathbf{X}) = \sum_{m=1}^{M} g(\mathbf{X}; T_m, \mu_m),\]<br>where each \(g\) represents a single regression tree. In BRTs, each tree is trained on the residuals of the previous ensemble, scaled by a learning rate to prevent overfitting. The process is deterministic and optimized via gradient descent.<br><br>BART takes this same additive structure but replaces the deterministic boosting process with a fully Bayesian formulation.<br><br></p><h2 class="mb-4">The BART Model</h2><p>In BART, the model for continuous outcomes is:<br>\[y_i = \sum_{m=1}^{M} g(\mathbf{X}_i; T_m, \mu_m) + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0, \sigma^2)\]<br>For binary responses, BART uses a probit link:<br>\[P(Y_i = 1 \mid \mathbf{X}_i) = \Phi\!\left(\sum_{m=1}^{M} g(\mathbf{X}_i; T_m, \mu_m)\right)\]<br>Each tree is weakly regularized, contributing only a small adjustment to the overall prediction. The prior enforces this behavior by penalizing deep trees and shrinking leaf values toward zero.<br></p><h2 class="mb-4">Priors and Regularization</h2><p>BART places priors over:<br></p><ul><li><p><strong>Tree structure:</strong> probability of node splits decreases with depth \(P(\text{split at depth } d) = \alpha (1 + d)^{-\beta}\)<br></p></li><li><p><strong>Terminal node values:</strong> \(\mu_{mj} \sim \mathcal{N}(0, \sigma_\mu^2)\)</p></li><li><p><strong>Error variance:</strong> \(\sigma^2 \sim \nu \lambda / \chi^2_\nu\)</p></li></ul><p>These priors act as built-in regularization, keeping trees shallow and smoothing the ensemble. Each tree captures a minor portion of the signal; together, they reconstruct complex functions.</p><h2 class="mb-4">Posterior Sampling</h2><p>Instead of optimizing via gradient descent, BART draws samples from the posterior:<br>\[P(\{T_m, \mu_m\}_{m=1}^{M}, \sigma^2 \mid y, X) \propto P(y \mid \{T_m, \mu_m\}, \sigma^2, X) \prod_{m=1}^{M} P(T_m) P(\mu_m \mid T_m) P(\sigma^2)\]<br>Sampling proceeds with a backfitting <strong>Gibbs sampler</strong>:<br></p><ol><li><p>Randomly select a tree.</p></li><li><p>Remove its contribution from the ensemble to compute residuals.</p></li><li><p>Update that tree to model the residuals.</p></li><li><p>Reinsert it and repeat for all trees.</p></li></ol><p>Over many iterations, the ensemble converges toward the posterior distribution of possible models.<br></p><h2 class="mb-4">Interpretation</h2><p>Predictions are obtained by averaging over posterior samples:<br>\[\hat{y}_i = \frac{1}{S} \sum_{s=1}^{S} f^{(s)}(\mathbf{X}_i)\]<br>and uncertainty naturally arises from the spread of \(f^{(s)}\).<br><br>In ecological applications, this posterior uncertainty is not noise, it reflects variation in model structure and parameter estimates that correspond to genuine uncertainty about species–environment relationships.<br><br>The lecture closed with live coding using `<a class="text-link-01" href="https://github.com/cjcarlson/embarcadero">embarcadero</a>` and `<a class="text-link-01" href="https://cran.r-project.org/web/packages/dbarts/index.html">dbarts</a>`, fitting BART to the Joshua tree dataset and visualizing how the sum-of-trees structure captured nonlinear temperature-precipitation responses far beyond the reach of traditional regression.<br></p><h1 class="mb-4 text-lg-center mt-lg-4 mb-lg-3">Interpreting Models — Partials and Spartials<br></h1><p>Once BART models are fitted, the focus shifts from prediction to interpretation. Unlike classical regressions with explicit coefficients, BART encodes relationships implicitly through the structure and frequency of tree splits. Understanding how predictors influence the response requires summarizing the model’s posterior behavior rather than inspecting parameter estimates.<br></p><h2 class="mb-4">Partial Dependence</h2><p>A <strong>partial dependence plot (PDP)</strong>&nbsp;shows how the predicted response changes with one predictor while averaging over all others. For a single variable \(x_j\), the partial function is defined as<br>\[\bar{f}(x_j) = \frac{1}{n} \sum_{i=1}^{n} f(x_j, \mathbf{X}_{i, -j}),\]<br>where \(\mathbf{X}_{i, -j}\) represents all predictors except \(x_j\). The function \(\bar{f}(x_j)\)expresses the <i>marginal effect</i>&nbsp;of \(x_j\) on the response, averaged over the distribution of other variables. <br><br>In BART, this quantity is computed over posterior samples, which allows uncertainty in model structure and parameter values to propagate through to the partial curve:<br>\[\hat{f}(x_j) = \frac{1}{S} \sum_{s=1}^{S} \frac{1}{n} \sum_{i=1}^{n} f^{(s)}(x_j, \mathbf{X}_{i, -j}),\]<br>where each \(f^{(s)}\) represents a different draw from the posterior distribution of trees.<br><br>Partial dependence plots reveal nonlinearities and thresholds that would be invisible in a linear model. In species distribution models, they often highlight ecological tolerances, such as a species’ response to temperature increasing up to an optimum before declining, or they can show interactions when combined with another variable.<br></p><h2 class="mb-4">Spatial Partials (“Spartials”)</h2><p>While partials describe marginal effects in predictor space, spatial partials project those effects back into geographic space. A spatial partial fixes one variable (e.g. temperature) and computes predictions across all locations, holding other predictors at their observed values. This produces a spatial map of the effect of that variable, isolating how the environment modifies the predicted probability of presence across space. Mathematically, this is the same as evaluating \(\bar{f}(x_j)\) over the raster grid:<br>\[\hat{y}_{r} = \frac{1}{S} \sum_{s=1}^{S} f^{(s)}(x_{j,r}, \mathbf{X}_{r, -j}),\]<br>for each raster cell \(r\). The result can be visualized as a continuous spatial gradient, showing, for example, where temperature or precipitation most strongly limits species presence.<br><br>These maps can reveal geographic zones of model sensitivity or environmental control, helping connect statistical relationships to real ecological constraints.<br></p><h2 class="mb-4">Uncertainty and Variable Importance</h2><p>Because BART is Bayesian, all interpretations include credible intervals. The spread of partial dependence curves or spatial partial maps reflects epistemic uncertainty, uncertainty in the model given limited data. Areas or variables with wide posterior intervals often signal sparse data or strong covariation among predictors.<br><br>Variable importance in BART is derived from the frequency and depth of tree splits. Predictors that are used frequently near the top of trees tend to explain more variance in the response. However, this frequency-based importance is diagnostic: it indicates where the model found structure.<br></p><h1 class="mb-4 text-lg-center mt-lg-4 mb-lg-3">Random-Intercept BART<br></h1><p>The final lecture introduced <strong>Random-Intercept BART (riBART)</strong>, an extension designed to handle structured ecological data. It is used in situations where observations are grouped by site, region, year, or any hierarchical unit that introduces dependence. Traditional BART assumes that all data points are independent, an assumption often violated in spatial or temporal studies. Random-intercept BART incorporates group-level variation directly into the model, combining the flexibility of tree ensembles with the structure of mixed models.<br></p><h2 class="mb-4">From Fixed to Random Effects</h2><p>In a linear mixed model, we write the response as:<br>\[y_i = \alpha_{g[i]} + \mathbf{x}_i^\top \boldsymbol{\beta} + \varepsilon_i,\]<br>where \(\alpha_{g[i]}\) is a random intercept for group \(g[i]\) (e.g. site or region), drawn from a group-level distribution such as<br>\[\alpha_{g[i]} \sim \text{Normal}(0, \sigma_\alpha^2).\]<br>This accounts for differences in baseline response between groups without estimating a separate parameter for each one. Random-intercept BART extends this logic to the nonparametric setting:<br>\[y_i = f(\mathbf{x}_i) + a_{g[i]} + \varepsilon_i,\]<br>where \(f(\mathbf{x}_i)\) is the sum-of-trees prediction, and \(a_{g[i]}\) is a random effect that captures structured residual variation. Both components are estimated jointly in the Bayesian framework, allowing uncertainty in \(f\) and \(a\) to interact naturally.<br><br>In hierarchical ecological data, such as multiple observations per site, spatial clusters of plots, or repeated surveys across years, this structure is very important. Without it, models can conflate within-group correlation with environmental effects, leading to overconfident estimates and inflated variable importance.<br></p><h2 class="mb-4">Posterior Estimation</h2><p>The random effects \(a_{g[i]}\) are treated as latent parameters and updated within the Gibbs sampler alongside the tree structures. After integrating over posterior draws, predictions take the form:<br>\[\hat{y}_i = \frac{1}{S} \sum_{s=1}^{S} \big( f^{(s)}(\mathbf{x}_i) + a^{(s)}_{g[i]} \big),\]<br>with corresponding credible intervals derived from the full posterior sample. <br><br>This formulation naturally partitions uncertainty into two components, variation in the environmental response $f$ and variation attributable to group-level heterogeneity \(a\).<br><br>In practice, the addition of random intercepts allows the model to fit local deviations without distorting global relationships. For example, two sites might experience the same temperature and precipitation but differ in baseline suitability due to unmeasured soil conditions or sampling effects; the random effect absorbs this difference.<br></p><h2 class="mb-4">Application to the Joshua Tree Dataset</h2><p>Using <strong>riBART</strong>, we modeled Joshua tree presence across multiple U.S. states. Each state received its own random intercept, capturing regional biases not explained by climate alone. This formulation revealed subtle differences in baseline occupancy probabilities: Nevada and California showing higher intercepts than Utah, while the main environmental response (temperature-precipitation curvature) remained consistent across regions. The resulting model combined three strengths:<br></p><ol><li><p><strong>Flexibility</strong>: Nonlinear environmental responses via tree ensembles</p></li><li><p><strong>Structure:</strong> Hierarchical grouping via random intercepts</p></li><li><p><strong>Uncertainty quantification:</strong> Full posterior inference for both components</p></li></ol><p>Together, these features allow BART to move beyond simple species-environment relationships, modeling ecological data in their true hierarchical and uncertain form.</p><div></div></div></div></div></div><!-- bloc-146 END --><!-- ScrollToTop Button --><button aria-label="Scroll to top button" class="bloc-button btn btn-d scrollToTop" onclick="scrollToTarget('1',this)"><svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 32 32"><path class="scroll-to-top-btn-icon" d="M30,22.656l-14-13-14,13"/></svg></button><!-- ScrollToTop Button END--><!-- pencil-footer --><div class="bloc l-bloc" id="pencil-footer"><div class="container bloc-sm bloc-sm-lg"><div class="row"><div class="col-md-6 text-start order-1 order-md-0"><p class="mb-0 footer-brand">© Copyright - Gilles Colling</p></div><div class="col-md-6 text-start footer-right mb-md-0 mb-sm-3 mb-3"><a href="../contact/" class="a-btn text-link-01 p-md">Contact</a></div></div></div></div><!-- pencil-footer END --></div><!-- Main container END -->  <!-- Additional JS --><script src="../js/bootstrap.bundle.min.js?7375"></script><script src="../js/blocs.min.js?2120"></script><script src="../js/lazysizes.min.js" defer></script><script src="../js/tables.js?8847"></script><!-- Additional JS END --><!-- KaTeX CSS --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.css"><!-- KaTeX JS (order matters: katex -> auto-render) --><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js"></script><script>(function () {  // pick the smallest container that holds your article content  var ROOT = document.querySelector('.page-content, main, .bloc-content') || document.body;  function haveMath(el){  // quick heuristic: do we see $$...$$ or $...$?  return /\$\$[\s\S]+?\$\$|\$[^$\n]+\$/.test(el.textContent);  }  function doRender(){  if (typeof window.renderMathInElement !== 'function') {  console.warn('[KaTeX] renderMathInElement not ready yet.');  return false;  }  try {  window.renderMathInElement(ROOT, { delimiters: [ {left: "$$", right: "$$", display: true}, {left: "$",  right: "$",  display: false}, {left: "\\(", right: "\\)", display: false}, {left: "\\[", right: "\\]", display: true} ], throwOnError: false, ignoredTags: ["script","noscript","style","textarea","pre","code"]  });  console.log('[KaTeX] render complete.');  return true;  } catch (e) {  console.error('[KaTeX] render error:', e);  return false;  }  }  // try at all the safe moments + a small retry  function boot(){  if (!haveMath(ROOT)) { console.log('[KaTeX] no math found (yet).'); }  if (!doRender()) setTimeout(doRender, 100); // retry once after a tick  }  if (document.readyState === 'complete' || document.readyState === 'interactive') {  boot();  } else {  document.addEventListener('DOMContentLoaded', boot, {once:true});  }  window.addEventListener('load', boot);  // Re-render if Blocs injects/changes content after load  var mo = new MutationObserver(function(){  clearTimeout(window.__katexMO);  window.__katexMO = setTimeout(doRender, 50);  });  mo.observe(ROOT, {childList:true, subtree:true});})();</script><!-- Optional: mobile-friendly wrapping; can live in your global CSS instead --><style>.katex { white-space: normal !important; overflow-wrap: anywhere !important; word-break: normal !important; }.katex-display { max-width: 100%; overflow-x: auto; -webkit-overflow-scrolling: touch; padding: .25em 0; }</style></body></html>